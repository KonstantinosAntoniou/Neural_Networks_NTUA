# NEURAL NETWORKS - NTUA ECE

This repository hosts the exercises for Neural Networks course of NTUA ECE.

## Collaborators

- [Anastasakis Zacharias](https://github.com/ZachariasAnastasakis)
- [Antoniou Konstantinos](https://github.com/KonstantinosAntoniou)
- [Vakis Michalis](https://github.com/mvaki)

## Lab1 - Supervised Learning

The goal of this exercise was to evaluate supervised machine learning algorithms on datasets taken by [UCI Machine Learning repository](https://archive.ics.uci.edu/ml/index.php) using data preprocessing techniques and fine tuning of the parameters of each algorithm. The datasets we experimented on are:

- [LSVT Voice Rehabilitation DataSet](http://archive.ics.uci.edu/ml/datasets/LSVT+Voice+Rehabilitation)
- [Isolet Dataset](https://archive.ics.uci.edu/ml/datasets/isolet)

## Lab2 - Unsupervised Learning

The goal of this exercise was to create a content based movie recommender focusing on the corpuses of the movies taken from [Carnegie Mellon Movie Summary Corpus](http://www.cs.cmu.edu/~ark/personas/). Afterwards we use [Somoclu's](https://somoclu.readthedocs.io/en/stable/index.html) library in order to create Self Organizing Maps (SOMs), clustering the movies with respect to their genres with the help of KMeans algorithm.

## Lab3 - Deep Learning

The goal of this exercise was to familiarize ourselves with Tensorflow2, Keras and Deep Neural Network Architectures. The task in particular was to classify images from a subset of [CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) using transfer learning on some frequently used [architectures](https://keras.io/api/applications/). Moreover for this task we created some models "from scratch". After evaluating the performance of these models we tried to optimize them in order to tackle problems like Overfitting, Memory Usage and Training time.

## Lab4 - Reinforcement Learning

 The goal of this exercise was to familiarize ourselves with RL algorithms. The task was to create a smart agent capable of playing the Atari-2600 game UpNDown. We tested a variety of environments and RL algorithms and chose the combination that provided the best results. Furthermore we recorded our best agent playing the game.

